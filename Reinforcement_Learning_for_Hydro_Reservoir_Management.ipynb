{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMQMFsIEdeu+fwSqSxF80VX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moushumipriya/HydroPowerRL-Reinforcement-Learning-for-Hydro-Reservoir-Management/blob/main/Reinforcement_Learning_for_Hydro_Reservoir_Management.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym torch torchvision torchaudio pandas numpy matplotlib tqdm\n"
      ],
      "metadata": {
        "id": "q6BTUpOKOpdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import gym\n",
        "from gym import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "def generate_synthetic_inflow(start_dt, end_dt, seed=42, base_capacity=1e6):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    hours = int((end_dt - start_dt).total_seconds() // 3600) + 1\n",
        "    times = [start_dt + timedelta(hours=i) for i in range(hours)]\n",
        "    day_of_year = np.array([t.timetuple().tm_yday for t in times])\n",
        "    seasonal = 0.5 + 0.5 * np.sin(2 * np.pi * (day_of_year / 365.0))\n",
        "    noise = rng.normal(scale=0.1, size=hours)\n",
        "    storms = rng.choice([0,1], size=hours, p=[0.98, 0.02]) * rng.exponential(scale=2.0, size=hours)\n",
        "    inflow = np.maximum(0.0, seasonal + 0.2*noise + storms)\n",
        "    inflow_cum = inflow * base_capacity * 0.001\n",
        "    df = pd.DataFrame({'inflow': inflow_cum}, index=times)\n",
        "    df.index.name = 'time'\n",
        "    price = (\n",
        "        20\n",
        "        + 5 * np.sin(2 * np.pi * (np.array([t.hour for t in times]) / 24.0))\n",
        "        + 3 * np.cos(2 * np.pi * (np.array([t.timetuple().tm_yday for t in times]) / 7.0))\n",
        "        + rng.normal(0, 1, size=hours)\n",
        "    )\n",
        "    df['price'] = np.maximum(0.1, price)\n",
        "    df['hour'] = [t.hour for t in times]\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "IVOBg9PEOpfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HydroEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df, capacity=1e6, init_level_frac=0.6, min_safe_frac=0.1, max_safe_frac=0.95):\n",
        "        super().__init__()\n",
        "        self.df = df.reset_index()\n",
        "        self.capacity = capacity\n",
        "        self.init_level = capacity * init_level_frac\n",
        "        self.min_safe = capacity * min_safe_frac\n",
        "        self.max_safe = capacity * max_safe_frac\n",
        "        self.t = 0\n",
        "        self.action_space = spaces.Discrete(3)  # 3 action levels: low, medium, high release\n",
        "        low = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
        "        high = np.array([1.0, np.finfo(np.float32).max, np.finfo(np.float32).max, 23.0], dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 0\n",
        "        self.level = self.init_level\n",
        "        self.prev_release = 0.0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.df.loc[self.t]\n",
        "        inflow = float(row['inflow'])\n",
        "        price = float(row['price'])\n",
        "        hour = int(row['hour'])\n",
        "        release_frac = {0: 0.005, 1: 0.02, 2: 0.05}[int(action)]\n",
        "        release = release_frac * self.capacity\n",
        "        next_level = np.clip(self.level + inflow - release, 0.0, self.capacity)\n",
        "\n",
        "        # Calculate power and revenue\n",
        "        head = 50.0  # height in meters\n",
        "        rho = 1000.0  # water density kg/m3\n",
        "        g = 9.81  # gravity\n",
        "        eta = 0.9  # efficiency\n",
        "        power_MW = eta * rho * g * release * head / 1e6\n",
        "        revenue = power_MW * price\n",
        "\n",
        "        # Penalties\n",
        "        penalty = 0.0\n",
        "        if next_level < self.min_safe:\n",
        "            penalty -= 1e4\n",
        "        if next_level > self.max_safe:\n",
        "            penalty -= 5e4\n",
        "        penalty -= 1e2 * abs(release - self.prev_release) / self.capacity\n",
        "\n",
        "        reward = revenue + penalty\n",
        "        self.prev_release = release\n",
        "        self.level = next_level\n",
        "        self.t += 1\n",
        "        done = self.t >= len(self.df)\n",
        "        return self._get_obs(), float(reward), done, {'revenue': revenue}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        row = self.df.loc[self.t]\n",
        "        inflow = float(row['inflow'])\n",
        "        price = float(row['price'])\n",
        "        hour = int(row['hour'])\n",
        "        return np.array([self.level/self.capacity, inflow, price, hour], dtype=np.float32)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"Step: {self.t}, Water Level: {self.level:.2f}, Prev Release: {self.prev_release:.2f}\")\n"
      ],
      "metadata": {
        "id": "0t6z2t45Opin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, obs_size, n_actions, hidden=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, n_actions)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=100000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    def push(self, *args):\n",
        "        self.buffer.append(Transition(*args))\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        return Transition(*zip(*batch))\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "etw_5_IBOplI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(env, num_episodes=100, max_steps=500, batch_size=64, gamma=0.99, lr=1e-3):\n",
        "    obs_size = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    net = DQN(obs_size, n_actions).to(device)\n",
        "    target_net = DQN(obs_size, n_actions).to(device)\n",
        "    target_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "    replay = ReplayBuffer(100000)\n",
        "\n",
        "    epsilon = 1.0\n",
        "    eps_min = 0.05\n",
        "    eps_decay = 0.995\n",
        "    sync_target_frames = 1000\n",
        "    total_steps = 0\n",
        "    all_rewards = []\n",
        "    losses = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        for step in range(max_steps):\n",
        "            total_steps += 1\n",
        "            state_v = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                q_vals = net(state_v)\n",
        "                action = int(torch.argmax(q_vals, dim=1).item())\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            replay.push(state, action, reward, next_state, done)\n",
        "            ep_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if len(replay) >= batch_size:\n",
        "                batch = replay.sample(batch_size)\n",
        "                states = torch.tensor(batch.state, dtype=torch.float32).to(device)\n",
        "                actions = torch.tensor(batch.action, dtype=torch.int64).unsqueeze(1).to(device)\n",
        "                rewards = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "                next_states = torch.tensor(batch.next_state, dtype=torch.float32).to(device)\n",
        "                dones = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "                q_values = net(states).gather(1, actions)\n",
        "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1).detach()\n",
        "                expected_q = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "                loss = nn.functional.mse_loss(q_values, expected_q)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            if total_steps % sync_target_frames == 0:\n",
        "                target_net.load_state_dict(net.state_dict())\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        epsilon = max(eps_min, epsilon * eps_decay)\n",
        "        all_rewards.append(ep_reward)\n",
        "        print(f\"Episode {ep+1}/{num_episodes}, reward: {ep_reward:.1f}, epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    return net, all_rewards, losses\n"
      ],
      "metadata": {
        "id": "UQefgtSoOpnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_dt = datetime(2024, 1, 1)\n",
        "end_dt = datetime(2024, 2, 1)  # 1 month hourly data\n",
        "\n",
        "df = generate_synthetic_inflow(start_dt, end_dt)\n",
        "print(df.head())\n",
        "\n",
        "env = HydroEnv(df, capacity=1e6)\n",
        "\n",
        "net, rewards, losses = train_dqn(env, num_episodes=5, max_steps=200, batch_size=32)\n",
        "\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.title('Training Reward per Episode')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WBA_dOWZOpqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(df.index, df['inflow'], label='Inflow (cubic meters)')\n",
        "plt.plot(df.index, df['price'], label='Electricity Price (NOK)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Synthetic Inflow and Electricity Price over Time')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Synthetic data generated for hourly inflow and price covering 1 month.\")\n"
      ],
      "metadata": {
        "id": "v437NpruOptH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.title('Training Reward per Episode')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training finished over {len(rewards)} episodes.\")\n",
        "print(f\"Final episode reward: {rewards[-1]:.2f}\")\n",
        "print(f\"Mean reward last 10 episodes: {np.mean(rewards[-10:]):.2f}\")\n"
      ],
      "metadata": {
        "id": "n5LfWq-tOpvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('DQN Training Loss over Time')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Loss shows how well the agent is learning to predict Q-values.\")\n"
      ],
      "metadata": {
        "id": "TQpJYQ4aOpyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "levels = []\n",
        "releases = []\n",
        "actions_taken = []\n",
        "\n",
        "for _ in range(len(df)-1):\n",
        "    state_v = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    q_vals = net(state_v)\n",
        "    action = int(torch.argmax(q_vals, dim=1).item())\n",
        "    _, reward, done, info = env.step(action)\n",
        "    levels.append(env.level)\n",
        "    releases.append(env.prev_release)\n",
        "    actions_taken.append(action)\n",
        "    state = env._get_obs()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(levels, label='Reservoir Water Level')\n",
        "plt.plot(releases, label='Water Release')\n",
        "plt.xlabel('Time Step (hour)')\n",
        "plt.ylabel('Volume (cubic meters)')\n",
        "plt.title('Reservoir Water Level and Release After Training')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Water level and release decisions taken by trained agent over 1 month.\")\n"
      ],
      "metadata": {
        "id": "TnFo1Z99P5d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hydro Reservoir RL Project Summary\n",
        "\n",
        "In this notebook, we have:\n",
        "\n",
        "- Generated synthetic hourly inflow and electricity price data for one month.\n",
        "- Created a Gym environment simulating a hydro reservoir management scenario.\n",
        "- Built and trained a Deep Q-Network (DQN) agent to optimize water release decisions.\n",
        "- Evaluated the agent's performance using reward and loss plots.\n",
        "- Visualized the agent's reservoir management behavior after training.\n",
        "\n",
        "This step-by-step approach demonstrates a practical application of reinforcement learning in energy management.\n",
        "\n"
      ],
      "metadata": {
        "id": "-8O_DCeKQJXf"
      }
    }
  ]
}